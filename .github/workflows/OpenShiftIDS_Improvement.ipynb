{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLGa_FEt9J8d"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from os.path import exists\n",
        "from os import mkdir\n",
        "from numpy import unique\n",
        "from numpy.random import rand\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.metrics import *\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we must import the datasets we plan to use. Be sure to confirm that the training dataset is larger than the testing dataset. Download the datasets from https://www.kaggle.com/datasets/mrwellsdavid/unsw-nb15."
      ],
      "metadata": {
        "id": "rGY5_Q4rgbQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainDS = pd.read_csv(\"UNSW_NB15_training-set.csv\")\n",
        "testDS = pd.read_csv(\"UNSW_NB15_testing-set.csv\")"
      ],
      "metadata": {
        "id": "Sfx_KBfU9SST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDS.head(5)"
      ],
      "metadata": {
        "id": "x2Pooo8R961_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testDS.head(5)"
      ],
      "metadata": {
        "id": "rdnCKW45-C97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDS.shape"
      ],
      "metadata": {
        "id": "odRhcm7TVB77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testDS.shape"
      ],
      "metadata": {
        "id": "YoR8c62ZVEte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the disparities in the different categories, we can count all the categories in each dataset."
      ],
      "metadata": {
        "id": "1tYXOEsJKaQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CountCategories(dataset, datasetLabel):\n",
        "    print(f'\\n\\nFrequency Statistics of {datasetLabel} dataset...')\n",
        "    print(\"------------------------------------------------------\")\n",
        "    catList = list(dataset[\"attack_cat\"])\n",
        "    uniqueCat = list(dataset[\"attack_cat\"].unique())\n",
        "    uniqueCatCount = [0] * len(uniqueCat)\n",
        "    print(uniqueCat)\n",
        "\n",
        "    for category in catList:\n",
        "        for i in range(len(uniqueCat)):\n",
        "            if uniqueCat[i] == category:\n",
        "                uniqueCatCount[i] += 1\n",
        "\n",
        "    for i in range(len(uniqueCat)):\n",
        "        print(f'{uniqueCat[i]}: {uniqueCatCount[i]}')\n",
        "\n",
        "    print(f'Total: {sum(uniqueCatCount)}')"
      ],
      "metadata": {
        "id": "IOfCoSrzKD7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CountCategories(trainDS, \"Training\")"
      ],
      "metadata": {
        "id": "3n4v_KxTKTsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CountCategories(testDS, \"Testing\")"
      ],
      "metadata": {
        "id": "C8DffD87KtAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Datasets have been imported and now it is time to Explore what figures we have here. First, lets inumerate all string values."
      ],
      "metadata": {
        "id": "MB06i7-fBqcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateNumCol(column, index, DF, uniqueItems):\n",
        "    rowCount = len(DF.axes[0])\n",
        "    newColIndex = index + 1\n",
        "    DF.insert(newColIndex, column + \"Num\", [0] * rowCount, True)\n",
        "\n",
        "    for row in range(rowCount):\n",
        "        curItem = DF.iloc[row, index]\n",
        "        for itemIndex in range(len(uniqueItems)):\n",
        "            if uniqueItems[itemIndex] == curItem:\n",
        "                DF.iloc[row, newColIndex] = itemIndex\n",
        "                break"
      ],
      "metadata": {
        "id": "k4eF1tpqRQZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AddNumCol(column, trainDS, testDS):\n",
        "    uniqueTrain = list(trainDS[column].unique())\n",
        "    uniqueTest = list(testDS[column].unique())\n",
        "    uniqueItems = []\n",
        "\n",
        "    # List of unique values\n",
        "    for i in range(len(uniqueTrain)):\n",
        "        uniqueItems.append(uniqueTrain[i])\n",
        "\n",
        "    for i in range(len(uniqueTest)):\n",
        "        if uniqueTest[i] not in uniqueItems:\n",
        "            uniqueItems.append(uniqueTest[i])\n",
        "\n",
        "    originalIndex = trainDS.columns.get_loc(column)\n",
        "    CreateNumCol(column, originalIndex, trainDS, uniqueItems)\n",
        "    CreateNumCol(column, originalIndex, testDS, uniqueItems)"
      ],
      "metadata": {
        "id": "_r3qSFKvP35c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AddNumCol(\"proto\", trainDS, testDS)\n",
        "AddNumCol(\"service\", trainDS, testDS)\n",
        "AddNumCol(\"state\", trainDS, testDS)"
      ],
      "metadata": {
        "id": "KM9dd3MLTHhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDS.head(20)"
      ],
      "metadata": {
        "id": "OgRCoMTJJxdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testDS.head(20)"
      ],
      "metadata": {
        "id": "VxflJuYuUb_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all values enumerated, we can now begin wrangling some more. Lets get rid of the columns we do not need in both tables."
      ],
      "metadata": {
        "id": "gVAtQJtBW_zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unneededCol = ['id', 'proto', 'service', 'state', 'attack_cat']\n",
        "purgedTrainDS = trainDS.drop(unneededCol, axis=1)\n",
        "purgedTestDS = testDS.drop(unneededCol, axis=1)"
      ],
      "metadata": {
        "id": "hspoI0vuXH3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This now leaves only numerical values in both of our datasets. We can start normalizing our data to make it easier on the machine learning modules. For this specific dataset, we choose between MaxAbsScaler and MinMax Scaler."
      ],
      "metadata": {
        "id": "ijgvaj9TYVx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "purgedTrainDS.dtypes"
      ],
      "metadata": {
        "id": "9sB3f9P0e-0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTDS = list(purgedTrainDS)\n",
        "\n",
        "xTrain = purgedTrainDS.iloc[:, :-1]\n",
        "yTrain = purgedTrainDS.iloc[:, -1]\n",
        "\n",
        "xTest = purgedTestDS.iloc[:, :-1]\n",
        "yTest = purgedTestDS.iloc[:, -1]"
      ],
      "metadata": {
        "id": "UE8YQc7tZETT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minMax = MinMaxScaler()\n",
        "maxAbs = MaxAbsScaler()\n",
        "\n",
        "mmTrain = minMax.fit_transform(xTrain)\n",
        "mmTest = minMax.transform(xTest)\n",
        "\n",
        "maTrain = maxAbs.fit_transform(xTrain)\n",
        "maTest = maxAbs.transform(xTest)"
      ],
      "metadata": {
        "id": "1Um4ZmYkZFim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data has now been pruned and standarized, and is now ready for modeling. The below function will generate our Confusion Matrixes for us."
      ],
      "metadata": {
        "id": "K3Gf_wF0kEto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateCM(yTest, yPred, fullName, path, clf):\n",
        "    cm = confusion_matrix(yTest, yPred, labels=clf.classes_)\n",
        "    cmDisplay = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
        "    cmDisplay.plot()\n",
        "\n",
        "    fileName = path + fullName + \"_CM.png\"\n",
        "    cloneNum = 0\n",
        "    while True:\n",
        "        if exists(fileName):\n",
        "            cloneNum += 1\n",
        "            fileName = path + str(cloneNum) + \"_\" + fullName + \"_CM.png\"\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    plt.savefig(fileName, dpi=300)"
      ],
      "metadata": {
        "id": "94Z17VGbr6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function generates all numbers we will compare."
      ],
      "metadata": {
        "id": "dZaY1zsRgpZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ResultsReturn(yTest, yPred, name, clf, elapsed):\n",
        "    uniCount = len(unique(yTest))\n",
        "    accCount = accuracy_score(yTest, yPred, normalize=False)\n",
        "    accScore = round(accuracy_score(yTest, yPred) * 100, 2)\n",
        "    balAccScore = round(balanced_accuracy_score(yTest, yPred) * 100, 2)\n",
        "    zeroOneLossScore = round(zero_one_loss(yTest, yPred) * 100, 2)\n",
        "    zeroOneCount = zero_one_loss(yTest, yPred, normalize=False)\n",
        "    if uniCount > 2:\n",
        "        precScore = round(precision_score(yTest, yPred, average=\"macro\") * 100, 2)\n",
        "        recScore = round(recall_score(yTest, yPred, average=\"macro\") * 100, 2)\n",
        "        f1Score = round(f1_score(yTest, yPred, average=\"macro\") * 100, 2)\n",
        "    else:\n",
        "        precScore = round(precision_score(yTest, yPred, average=\"binary\") * 100, 2)\n",
        "        recScore = round(recall_score(yTest, yPred, average=\"binary\") * 100, 2)\n",
        "        f1Score = round(f1_score(yTest, yPred, average=\"binary\") * 100, 2)\n",
        "\n",
        "    return [\n",
        "        name, str(clf), uniCount, accCount, zeroOneCount, accScore, balAccScore,\n",
        "        precScore, recScore, f1Score, zeroOneLossScore, elapsed\n",
        "    ]"
      ],
      "metadata": {
        "id": "a3O1BnGkwCMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function prints all results as a CSV file."
      ],
      "metadata": {
        "id": "MAIh9TdjgutQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ExportCSV(allResults, fullName, path):\n",
        "    scoreListLabels = [\n",
        "        \"Name\", \"Classifier\", \"Unique_Classifications\", \"Correct_Predictions\",\n",
        "        \"Incorrect_Predictions\", \"Accuracy\", \"Balanced_Accuracy\", \"Precision\",\n",
        "        \"Recall\", \"F1_Score\", \"Loss\", \"Execution Time\"\n",
        "    ]\n",
        "\n",
        "    resultsDF = pd.DataFrame(allResults, columns=scoreListLabels)\n",
        "\n",
        "    # Filename\n",
        "    fileName = f'{path}{fullName}.csv'\n",
        "    cloneNum = 0\n",
        "    while True:\n",
        "        if exists(fileName):\n",
        "            cloneNum += 1\n",
        "            fileName = f'{path}{str(cloneNum)}_{fullName}.csv'\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    resultsDF.to_csv(fileName, index=False)"
      ],
      "metadata": {
        "id": "43n5I0O9Hq8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function will generate all the bar graphs we need for visulizations"
      ],
      "metadata": {
        "id": "GKSgrSPegyF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GenerateCharts(allResults, fullName, path):\n",
        "    scoreListLabels = [\n",
        "        \"Name\", \"Classifier\", \"Unique_Classifications\", \"Correct_Predictions\",\n",
        "        \"Incorrect_Predictions\", \"Accuracy\", \"Balanced_Accuracy\", \"Precision\",\n",
        "        \"Recall\", \"F1_Score\", \"Loss\", \"Execution Time\"\n",
        "    ]\n",
        "\n",
        "    resultsDF = pd.DataFrame(allResults, columns=scoreListLabels)\n",
        "    colorList = [rand(3)] * resultsDF.shape[0]\n",
        "\n",
        "    for i in range(3, len(scoreListLabels)):\n",
        "        resultsDF.plot(\n",
        "            x='Name',\n",
        "            y=scoreListLabels[i],\n",
        "            kind='bar',\n",
        "            color=colorList,\n",
        "            legend=None\n",
        "        )\n",
        "\n",
        "        # Filename\n",
        "        fileName = f'{path}{fullName}_{scoreListLabels[i]}.png'\n",
        "        cloneNum = 0\n",
        "        while True:\n",
        "            if exists(fileName):\n",
        "                cloneNum += 1\n",
        "                fileName = f'{path}{str(cloneNum)}_{fullName}_{scoreListLabels[i]}.png'\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Adjust the y-axis for visuals\n",
        "        yRange = resultsDF[scoreListLabels[i]]\n",
        "        bufferPerc = 0.25\n",
        "        buffer = (yRange.max() - yRange.min()) * bufferPerc + 0.05\n",
        "        yMin = yRange.min() - buffer\n",
        "        if yMin < 0:\n",
        "            yMin = 0\n",
        "        yMax = yRange.max() + buffer\n",
        "        plt.ylim((yMin, yMax))\n",
        "\n",
        "        # Fine tuning visuals\n",
        "        plt.title(scoreListLabels[i] + \" Comparison\")\n",
        "        plt.xlabel(\"Classifiers\")\n",
        "        plt.ylabel(\"Scores/Percentages\")\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.tight_layout()\n",
        "        for i, v, in enumerate(yRange.tolist()):\n",
        "            plt.text(i, v, str(v), ha=\"center\")\n",
        "\n",
        "        plt.savefig(fileName)"
      ],
      "metadata": {
        "id": "1Fcb7lT8-0Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function puts it all together and gets together everything we need for the model."
      ],
      "metadata": {
        "id": "VjvsUfOshJof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ModelResults(xTrain, yTrain, xTest, yTest, stdType, path):\n",
        "    # Principal Component Analysis\n",
        "    pcaTrain = PCA(n_components=15)\n",
        "    pcaTrain.fit(xTrain)\n",
        "\n",
        "    xTrain = pcaTrain.transform(xTrain)\n",
        "    xTest = pcaTrain.transform(xTest)\n",
        "\n",
        "    # Start Modeling\n",
        "    allResults = []\n",
        "    modelNames = [\n",
        "        \"Neural Network\", \"Random Forest\", \"Decision Tree\", \"Naive Bayes\",\n",
        "        \"K-Nearest Neighbor\"\n",
        "    ]\n",
        "\n",
        "    models = [\n",
        "        MLPClassifier(random_state=42, hidden_layer_sizes=(50, 20, 5,)),\n",
        "        RandomForestClassifier(random_state=42, n_estimators=50),\n",
        "        DecisionTreeClassifier(random_state=42),\n",
        "        GaussianNB(),\n",
        "        KNeighborsClassifier(n_neighbors=2)\n",
        "    ]\n",
        "\n",
        "    # Iterate through models\n",
        "    for name, clf in zip(modelNames, models):\n",
        "        print(f'Tailoring {name} for {stdType} Standard')\n",
        "        start = time()\n",
        "        clf.fit(xTrain, yTrain)\n",
        "        end = time()\n",
        "        elapsed = round(end - start, 6)\n",
        "        yPred = clf.predict(xTest)\n",
        "\n",
        "        # Results\n",
        "        fullName = f'{stdType}-{name}'\n",
        "        CreateCM(yTest, yPred, fullName, path, clf)\n",
        "        rr = ResultsReturn(yTest, yPred, fullName, clf, elapsed)\n",
        "        allResults.append(rr)\n",
        "\n",
        "    # Export all Visualizations\n",
        "    fullName = f'{stdType}'\n",
        "    ExportCSV(allResults, fullName, path)\n",
        "    GenerateCharts(allResults, fullName, path)"
      ],
      "metadata": {
        "id": "8mlakKHVl4ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathNum = 0\n",
        "path = \"OpenShift Results/\"\n",
        "while exists(path):\n",
        "    pathNum += 1\n",
        "    path = f'OpenShift Results {pathNum}/'\n",
        "mkdir(path)"
      ],
      "metadata": {
        "id": "SYw3YgmjOAKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ModelResults(mmTrain, yTrain, mmTest, yTest, \"Min Max\", path)\n",
        "ModelResults(maTrain, yTrain, maTest, yTest, \"Max Abs\", path)"
      ],
      "metadata": {
        "id": "vq_k-e0anCSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}